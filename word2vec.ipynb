{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is an approach to implement the word2vec paper (https://arxiv.org/abs/1301.3781) using GloVe embeddings (previously trained).\n",
        "Referred/inspiration:\n",
        "- [A Dummyâ€™s Guide to Word2Vec](https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673)\n",
        "- [Word2vec from Scratch](https://jaketae.github.io/study/word2vec/)\n",
        "\n",
        "# Additions: GloVe embeddins + Reddit dataset\n"
      ],
      "metadata": {
        "id": "-r9d4bDkf1f8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO_1wzRvfx2P"
      },
      "outputs": [],
      "source": [
        "!pip install gensim nltk datasets matplotlib umap-learn\n",
        "\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from datasets import load_dataset\n",
        "from sklearn.manifold import TSNE\n",
        "import umap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "zdhMBdmlf1Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained GloVe embeddings\n",
        "glove_vectors = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "id": "vpPohhN2f1T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Reddit dataset (subsample for efficiency)\n",
        "dataset = load_dataset(\"reddit\", split=\"train[:10%]\")"
      ],
      "metadata": {
        "id": "YHTh7mBPf1W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    return words"
      ],
      "metadata": {
        "id": "nBCuwkLbhEe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess Reddit comments\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "corpus = [preprocess_text(comment) for comment in dataset['body'] if isinstance(comment, str)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xaCs42lhEiO",
        "outputId": "51bffad4-16ee-4c88-e962-1c6ffd83f382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=corpus, vector_size=50, window=5, min_count=5, workers=4, sg=1)\n"
      ],
      "metadata": {
        "id": "ZhKlC8BfhEn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "word2vec_model.save(\"word2vec_reddit.model\")"
      ],
      "metadata": {
        "id": "XD48DYZ_hErT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Word2Vec model\n",
        "print(\"Words similar to 'reddit':\", word2vec_model.wv.most_similar(\"reddit\"))"
      ],
      "metadata": {
        "id": "OfVJB2wphEuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of embeddings\n",
        "def visualize_embeddings(model, num_words=100):\n",
        "    words = list(model.wv.index_to_key)[:num_words]\n",
        "    vectors = np.array([model.wv[word] for word in words])\n",
        "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "    vectors_2d = reducer.fit_transform(vectors)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], marker='o')\n",
        "\n",
        "    for word, (x, y) in zip(words, vectors_2d):\n",
        "        plt.text(x, y, word, fontsize=9)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "visualize_embeddings(word2vec_model)"
      ],
      "metadata": {
        "id": "5MV4uREVhEw4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}